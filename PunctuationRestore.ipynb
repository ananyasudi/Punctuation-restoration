{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osKe6FvzKlX2",
        "outputId": "b42d0349-2203-4f5e-a790-b5c6cba05f19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import csv\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeiIGUT_aPkY",
        "outputId": "0269bce8-270e-4134-9d42-7d733c56ff3b"
      },
      "outputs": [],
      "source": [
        "# move vocab out of the dataset class\n",
        "# use train.csv for generating vicab\n",
        "# divide train into train, val and test\n",
        "# get dataloaders for each\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "def clean(st):\n",
        "  s=st.lower()\n",
        "  s=re.sub(\"\\s(I'm)\\s\",\" I am \",s)\n",
        "  s=re.sub(\"\\s(don't)\\s\",\" do not \",s)\n",
        "  s=re.sub(\"\\s(can't)\\s\",\" can not \",s)\n",
        "  s=re.sub(\"\\s(doesn't)\\s\",\" do not \",s)\n",
        "  s=re.sub(\"\\s(won't)\\s\",\" will not \",s)\n",
        "  s=re.sub(\"\\s(it's)\\s\",\" it is \",s)\n",
        "  s=re.sub(\"[:`\\\"]\",\"\",s)\n",
        "  s=re.sub(\"'ve\",\" have\",s)\n",
        "  s=re.sub(\"\\.\",\". \",s)\n",
        "  return s\n",
        "def build_vocab(path):\n",
        "  # creating vocab\n",
        "  # create train data [[[tok1,tok2,tok3..],[op1,op2,op3....]],[[],[]]..]\n",
        "  punct=[\".\",\",\",\"?\",\"None\"]\n",
        "  vocab=[]\n",
        "  data=[]\n",
        "  num_rows=0\n",
        "  with open(path, 'r') as f:\n",
        "    csvfile=csv.reader(f)\n",
        "    for line in csvfile:\n",
        "      num_rows+=1\n",
        "      if num_rows>1:\n",
        "        if line[1]!=\"\":\n",
        "          line[1]=clean(line[1])\n",
        "          data.append(line[1])\n",
        "          vocab+=nltk.word_tokenize(line[1].lower())\n",
        "\n",
        "  # print(len(set(vocab)))\n",
        "  vocab=set(vocab)\n",
        "  vocab={word:i for i,word in enumerate(vocab)}\n",
        "  l=len(vocab)\n",
        "  vocab['OOV']=l\n",
        "  vocab[\"<EOS>\"]=l+1\n",
        "  vocab[\"<BOS>\"]=l+2\n",
        "  vocab[\"<PAD>\"]=l+3\n",
        "  idx_to_word={i:word for word,i in vocab.items()}\n",
        "\n",
        "  punct={p:i for i,p in enumerate(punct)}\n",
        "  idx_to_punct={i:p for p,i in punct.items()}\n",
        "  maxi_len=0\n",
        "\n",
        "  print(len(data))\n",
        "  return vocab,punct,idx_to_word,idx_to_punct\n",
        "\n",
        "# dividing into train, test, val data\n",
        "def dividedata(path,vocab,punct,idx_to_word,idx_to_punct):\n",
        "\n",
        "  data=[]\n",
        "  num_rows=0\n",
        "  with open(path, 'r') as f:\n",
        "    csvfile=csv.reader(f)\n",
        "    for line in csvfile:\n",
        "      num_rows+=1\n",
        "      if num_rows>1:\n",
        "        if line[1]!=\"\":\n",
        "          line[1]=clean(line[1])\n",
        "          data.append(line[1])\n",
        "    tot_data=[]\n",
        "    maxi_len=0\n",
        "\n",
        "    for j in range(len(data)):\n",
        "      words=nltk.word_tokenize(data[j])\n",
        "      # print(\"words len \",len(words))\n",
        "      inp=[vocab[\"<BOS>\"]]\n",
        "      non=\"None\"\n",
        "      op=[punct[non]]\n",
        "      for i in range(len(words)):\n",
        "        if words[i] not in punct:\n",
        "          inp.append(vocab.get(words[i],vocab['OOV']))\n",
        "          if i+1<len(words) and words[i+1] in punct:\n",
        "            op.append(punct[words[i+1]])\n",
        "            continue\n",
        "          else:\n",
        "            op.append(punct[non])\n",
        "        elif words[i]==\".\":\n",
        "          inp.append(vocab[\"<EOS>\"])\n",
        "          op.append(punct[non])\n",
        "          if i<len(words)-1:\n",
        "            inp.append(vocab[\"<BOS>\"])\n",
        "            op.append(punct[non])\n",
        "      if len(inp)==len(op):\n",
        "        tot_data.append([inp,op])\n",
        "        maxi_len=max(maxi_len,len(inp))\n",
        "      else:\n",
        "        print(\"err\")\n",
        "    print(\"Number of samples : \",len(tot_data))\n",
        "    print(\"Each sample : \",len(tot_data[0]))\n",
        "    X=[x[0] for x in tot_data]\n",
        "    y=[y[1] for y in tot_data]\n",
        "\n",
        "\n",
        "\n",
        "    # print(len(X))\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.3, random_state=42)\n",
        "\n",
        "    print([idx_to_word[x] for x in X_val[0]])\n",
        "    print([idx_to_punct[x] for x in y_val[0]])\n",
        "    return X_train,y_train,X_val,y_val,X_test,y_test\n",
        "\n",
        "\n",
        "vocab,punct,idx_to_word,idx_to_punct=build_vocab('train.csv')\n",
        "X_train,y_train,X_val,y_val,X_test,y_test=dividedata('train.csv',vocab,punct,idx_to_word,idx_to_punct)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKlmUhJ-sEZI",
        "outputId": "fb724d8f-7b09-43b0-b29e-c8fb3dcd14d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: '.', 1: ',', 2: '?', 3: 'None'}\n"
          ]
        }
      ],
      "source": [
        "print(idx_to_punct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPUoSDMmrPbi",
        "outputId": "99a1b8ec-5314-45a5-acf0-e1ec29fc74b3"
      },
      "outputs": [],
      "source": [
        "# re-checking\n",
        "# print([idx_to_word[x] for x in X_val[0]])\n",
        "# print([idx_to_punct[x] for x in y_val[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_SBl9pQKmqH",
        "outputId": "2967c7d6-f4c2-4919-d119-63b98b678778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2806\n",
            "491\n",
            "211\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class dataset(Dataset):\n",
        "  x=[]\n",
        "  y=[]\n",
        "\n",
        "  def __init__(self,x,y):\n",
        "    self.x=x\n",
        "    self.y=y\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.x)\n",
        "\n",
        "  def __getitem__(self,index:int):\n",
        "    try:\n",
        "      _=self.x[index]\n",
        "      return [self.x[index],self.y[index]]\n",
        "    except:\n",
        "      print(index)\n",
        "\n",
        "\n",
        "train_obj=dataset(X_train,y_train)\n",
        "val_obj=dataset(X_val,y_val)\n",
        "test_obj=dataset(X_test,y_test)\n",
        "print(len(train_obj.x))\n",
        "print(len(val_obj.x))\n",
        "print(len(test_obj.x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znMjxSG_K29X",
        "outputId": "1d9c9a5f-fc40-435b-998a-268c78e5d3ba"
      },
      "outputs": [],
      "source": [
        "# issues with vocab\n",
        "# 1. words joined by . (hope.however, died.then)- split using period\n",
        "# 2. numbers like 501, 404\n",
        "# 3. phone numbers\n",
        "# 4. urls\n",
        "print(len(vocab))\n",
        "print(vocab)\n",
        "print(train_obj.x[0])\n",
        "print(train_obj.y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0LwIBVaK7Mb",
        "outputId": "7d45b0f7-23af-46c0-d58f-5b8cb304c504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of batch  64 2\n",
            "len of inputs :  64 131\n",
            "Feature batch shape: torch.Size([64, 579])\n",
            "Number of lengths 64\n",
            "Labels batch shape: 64 579\n",
            "Length of batch  32 2\n",
            "len of inputs :  32 400\n",
            "Feature batch shape: torch.Size([32, 469])\n",
            "Number of lengths 32\n",
            "Labels batch shape: 32 469\n",
            "Length of batch  32 2\n",
            "len of inputs :  32 123\n",
            "Feature batch shape: torch.Size([32, 814])\n",
            "Number of lengths 32\n",
            "Labels batch shape: 32 814\n"
          ]
        }
      ],
      "source": [
        "# divide data into train, val, and test\n",
        "\n",
        "pad_idx=vocab['<PAD>']\n",
        "pad_idx_punct=punct[\"None\"]\n",
        "def custom_collate(batch):\n",
        "  print(\"Length of batch \",len(batch), len(batch[0]))\n",
        "  inputs=[torch.tensor(elem[0]) for elem in batch]\n",
        "  lengths=[len(elem[0]) for elem in batch]\n",
        "  print(\"len of inputs : \",len(inputs), len(inputs[0]))\n",
        "  labels=[torch.tensor(elem[1]) for elem in batch]\n",
        "  inputs=pad_sequence(inputs,batch_first=True,padding_value=pad_idx)\n",
        "  labels=pad_sequence(labels,batch_first=True,padding_value=pad_idx_punct)\n",
        "  return [inputs,lengths,labels]\n",
        "\n",
        "train_dataloader=DataLoader(train_obj,batch_size=64,shuffle=True,collate_fn=custom_collate)\n",
        "trainx,lenx,trainy=next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {trainx.size()}\")\n",
        "print(f\"Number of lengths {len(lenx)}\")\n",
        "print(f\"Labels batch shape: {len(trainy)} {len(trainy[0])}\")\n",
        "\n",
        "\n",
        "val_dataloader=DataLoader(val_obj,batch_size=32,shuffle=True,collate_fn=custom_collate)\n",
        "valx,lenx,valy=next(iter(val_dataloader))\n",
        "print(f\"Feature batch shape: {valx.size()}\")\n",
        "print(f\"Number of lengths {len(lenx)}\")\n",
        "print(f\"Labels batch shape: {len(valy)} {len(valy[0])}\")\n",
        "\n",
        "\n",
        "\n",
        "test_dataloader=DataLoader(test_obj,batch_size=32,shuffle=True,collate_fn=custom_collate)\n",
        "testx,lenx,testy=next(iter(test_dataloader))\n",
        "print(f\"Feature batch shape: {testx.size()}\")\n",
        "print(f\"Number of lengths {len(lenx)}\")\n",
        "print(f\"Labels batch shape: {len(testy)} {len(testy[0])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl--VxhUx6qJ",
        "outputId": "d9f951e4-f20a-44f9-90cd-83c25515c0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        }
      ],
      "source": [
        "print(len(val_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fEZ0oS5jQQjx"
      },
      "outputs": [],
      "source": [
        "from torch import nn,tensor\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class lstm_model(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_dim,hidden_size,out_size):\n",
        "    super(lstm_model, self).__init__()\n",
        "    self.embedding_dim=embed_dim\n",
        "    self.vocab_size=vocab_size\n",
        "    self.output_size=out_size\n",
        "    self.hidden_dim=hidden_size\n",
        "    self.embeddings=nn.Embedding(vocab_size,embed_dim)\n",
        "    self.bilstm=nn.LSTM(embed_dim,hidden_size,bidirectional=True,batch_first=True)\n",
        "    self.fc=nn.Linear(2*hidden_size,out_size)\n",
        "  def forward(self,inputs,lengths):\n",
        "    print(f\"Inputs: {inputs.shape}\")\n",
        "    embeds=self.embeddings(inputs)\n",
        "    # batchsize * seqlen * embded_dim\n",
        "    print(f\"Embeddings shape : {embeds.shape}\")\n",
        "\n",
        "    if len(lengths)>1:\n",
        "      packed_input = pack_padded_sequence(embeds, lengths, batch_first=True,enforce_sorted=False)\n",
        "\n",
        "      # out,_=self.bilstm(embeds)\n",
        "      packed_output, _ = self.bilstm(packed_input)\n",
        "      output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "    else:\n",
        "      output,_=self.bilstm(embeds)\n",
        "    print(f\"LSTM {output.shape}\")\n",
        "\n",
        "    # batchsize * seqlen * (2. hidden dim)\n",
        "    prob=self.fc(output)\n",
        "    print(f\"out shape : {prob.shape}\")\n",
        "\n",
        "    # batchsize * seqlen * out_size\n",
        "    return prob\n",
        "\n",
        "vocab_size=len(vocab)\n",
        "embed_dim=120\n",
        "hidden_dim=150\n",
        "out_size=len(punct)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "epochs = 2\n",
        "learning_rate = 0.1\n",
        "model=lstm_model(vocab_size,embed_dim,hidden_dim,out_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "a=torch.nn.Softmax(dim=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlenAnCwWnX_"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "e_num=0\n",
        "target_names=[idx_to_punct[i] for i in range(len(punct))]\n",
        "for epoch in range(epochs):\n",
        "  train_loss = 0\n",
        "  model.train()\n",
        "  num_batch=0\n",
        "  all_pred=[]\n",
        "  all_actual=[]\n",
        "\n",
        "  for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "    x_train,lens, y_train = batch\n",
        "    optimizer.zero_grad()# clears the gradients from prev iteration\n",
        "    # print('train',x_train.shape)\n",
        "    output = model(x_train,lens) # forward pass\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    output=output.view(-1,output.shape[2])\n",
        "    output = a(output)\n",
        "\n",
        "    # print('test',output.shape,y_train.shape)\n",
        "    loss = loss_fn(output, y_train.view(-1)) # calculate loss\n",
        "\n",
        "    #backprop\n",
        "    loss.backward() # compute gradients\n",
        "    optimizer.step() # update parameters\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    num_batch+=1\n",
        "\n",
        "    val_loss = 0\n",
        "\n",
        "\n",
        "  # Validation loop\n",
        "  val_loss = 0\n",
        "  model.eval()\n",
        "  for i, (x_val,lens, y_val) in enumerate(val_dataloader):\n",
        "    output = model(tensor(x_val),lens)\n",
        "    output=output.view(-1,output.shape[2])\n",
        "    output=a(output)\n",
        "    all_pred+=[torch.argmax(output[i]) for i in range(len(output))]\n",
        "    all_actual+=y_val.view(-1)\n",
        "    loss=loss_fn(output,y_val.view(-1))\n",
        "    # all_actual+=list(chain.from_iterable(y_val))\n",
        "    # loss = loss_fn(output, tensor(list(chain.from_iterable(y_val))))\n",
        "    val_loss += loss.item()\n",
        "  print(classification_report(all_actual, all_pred, target_names=target_names))\n",
        "  print(f'Epoch {e_num} \\t\\t Training Loss: {train_loss / len(train_dataloader)} \\t\\t Validation Loss: {val_loss / len(val_dataloader)}')\n",
        "  e_num+=1\n",
        "  print(train_loss/len(train_dataloader))\n",
        "  print(f\"Number of batches {num_batch}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddMOfJv4v-A0",
        "outputId": "3068ebc5-2291-4b41-fef4-9b09624558b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of batch  32 2\n",
            "len of inputs :  32 391\n",
            "Inputs: torch.Size([32, 416])\n",
            "Embeddings shape : torch.Size([32, 416, 120])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-56-2cc618d167c7>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  output = model(tensor(x_test),lens)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM torch.Size([32, 416, 300])\n",
            "out shape : torch.Size([32, 416, 4])\n",
            "Length of batch  32 2\n",
            "len of inputs :  32 115\n",
            "Inputs: torch.Size([32, 534])\n",
            "Embeddings shape : torch.Size([32, 534, 120])\n",
            "LSTM torch.Size([32, 534, 300])\n",
            "out shape : torch.Size([32, 534, 4])\n",
            "Length of batch  32 2\n",
            "len of inputs :  32 242\n",
            "Inputs: torch.Size([32, 477])\n",
            "Embeddings shape : torch.Size([32, 477, 120])\n",
            "LSTM torch.Size([32, 477, 300])\n",
            "out shape : torch.Size([32, 477, 4])\n",
            "Length of batch  32 2\n",
            "len of inputs :  32 202\n",
            "Inputs: torch.Size([32, 494])\n",
            "Embeddings shape : torch.Size([32, 494, 120])\n",
            "LSTM torch.Size([32, 494, 300])\n",
            "out shape : torch.Size([32, 494, 4])\n",
            "Length of batch  32 2\n",
            "len of inputs :  32 239\n",
            "Inputs: torch.Size([32, 1020])\n",
            "Embeddings shape : torch.Size([32, 1020, 120])\n",
            "LSTM torch.Size([32, 1020, 300])\n",
            "out shape : torch.Size([32, 1020, 4])\n",
            "Length of batch  32 2\n",
            "len of inputs :  32 127\n",
            "Inputs: torch.Size([32, 814])\n",
            "Embeddings shape : torch.Size([32, 814, 120])\n",
            "LSTM torch.Size([32, 814, 300])\n",
            "out shape : torch.Size([32, 814, 4])\n",
            "Length of batch  19 2\n",
            "len of inputs :  19 78\n",
            "Inputs: torch.Size([19, 437])\n",
            "Embeddings shape : torch.Size([19, 437, 120])\n",
            "LSTM torch.Size([19, 437, 300])\n",
            "out shape : torch.Size([19, 437, 4])\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           .       0.00      0.25      0.01      1742\n",
            "           ,       0.04      0.19      0.07      1429\n",
            "           ?       0.01      0.07      0.01       163\n",
            "        None       0.90      0.09      0.16    125129\n",
            "\n",
            "    accuracy                           0.09    128463\n",
            "   macro avg       0.24      0.15      0.06    128463\n",
            "weighted avg       0.88      0.09      0.16    128463\n",
            "\n",
            " Test Loss: 1.382284470966884\n"
          ]
        }
      ],
      "source": [
        "test_loss=0\n",
        "all_actual=[]\n",
        "all_pred=[]\n",
        "for i, (x_test,lens, y_test) in enumerate(test_dataloader):\n",
        "    output = model(tensor(x_test),lens)\n",
        "    output=output.view(-1,output.shape[2])\n",
        "    output=a(output)\n",
        "    all_pred+=[torch.argmax(output[i]) for i in range(len(output))]\n",
        "    all_actual+=y_test.view(-1)\n",
        "    loss=loss_fn(output,y_test.view(-1))\n",
        "    # all_actual+=list(chain.from_iterable(y_test))\n",
        "    # loss = loss_fn(output, tensor(list(chain.from_iterable(y_test))))\n",
        "    test_loss += loss.item()\n",
        "print(classification_report(all_actual, all_pred, target_names=target_names))\n",
        "print(f' Test Loss: {test_loss / len(test_dataloader)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbUGpKyk4HZ8",
        "outputId": "ed229e50-afe9-441a-901e-b6264a340170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['model_state_dict', 'optimizer_state_dict', 'embedding_dim', 'hidden_dim', 'vocab_size', 'vocab', 'punct', 'idx_to_word', 'idx_to_punct'])\n"
          ]
        }
      ],
      "source": [
        "# torch.save({\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'embedding_dim': model.embedding_dim,\n",
        "#             'hidden_dim': model.hidden_dim,\n",
        "#             'vocab_size': vocab_size,\n",
        "#             'vocab':vocab,\n",
        "#             'punct':punct,\n",
        "#             'idx_to_word':idx_to_word,\n",
        "#             'idx_to_punct':idx_to_punct\n",
        "#             }, 'pre_trained_sst.pth')\n",
        "pre_trained=torch.load('./pre_trained_sst.pth')\n",
        "print(pre_trained.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVE5f1SCfjJg",
        "outputId": "41e2ba3d-e4c5-4daa-8c44-87d08ca9e476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs: torch.Size([1, 7])\n",
            "Embeddings shape : torch.Size([1, 7, 120])\n",
            "LSTM torch.Size([1, 7, 300])\n",
            "out shape : torch.Size([1, 7, 4])\n",
            "Hi . How . are , you I am . fine \n",
            "\n",
            "torch.Size([1, 7, 4])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "sentence='Hi How are you I am fine'\n",
        "twords=nltk.word_tokenize(sentence)\n",
        "words=[vocab[word] if word in vocab else vocab['OOV'] for word in twords]\n",
        "words=torch.tensor([words])\n",
        "lens=torch.tensor([len(words)])\n",
        "out=model(words, lens)\n",
        "out=a(out)\n",
        "punctuations=[]\n",
        "for probdist in out[0]:\n",
        "  index=torch.argmax(probdist)\n",
        "  punctuations.append(idx_to_punct[int(index)])\n",
        "for i in range(len(twords)):\n",
        "  print(twords[i],end=\" \")\n",
        "  if punctuations[i]!=\"None\":\n",
        "    print(f\"{punctuations[i]} \",end=\"\")\n",
        "print(\"\\n\")\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnSW8FTEgCjp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
